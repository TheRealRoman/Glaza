{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OewAky_cEtE4"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/qubvel/segmentation_models.pytorch\n",
        "# !pip install timm\n",
        "# !pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig3FuNnB6Cea"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import albumentations as A\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tqdm\n",
        "import segmentation_models_pytorch as smp\n",
        "from sklearn.model_selection import StratifiedKFold,KFold\n",
        "\n",
        "import tifffile as tiff\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlbcAFnz7vA7",
        "outputId": "2424b122-413b-437d-e05c-b8844f67f4da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "name_archive = 'train_dataset_mc.zip'\n",
        "os.system(f' cp \"drive/My Drive/{name_archive}\" \"{name_archive}\" ')\n",
        "os.system(f' unzip {name_archive} -d train_folder' )\n",
        "\n",
        "name_archive = 'test_dataset_mc2.zip'\n",
        "os.system(f' cp \"drive/My Drive/{name_archive}\" \"{name_archive}\" ')\n",
        "os.system(f' unzip {name_archive} -d test_folder' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRQgGmrDEU-n"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB7YzxpCQUcm"
      },
      "outputs": [],
      "source": [
        "def read_image(path: str) -> np.ndarray:\n",
        "    image = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "def parse_polygon(coordinates: dict, image_size: tuple, mode) -> np.ndarray:\n",
        "    mask = np.zeros(image_size, dtype=np.float32)\n",
        "    if len(coordinates) == 1:\n",
        "        points = [np.int32(coordinates)]\n",
        "        cv2.fillPoly(mask, points, 1)\n",
        "    else: \n",
        "        points = [np.int32([coordinates[0]])] \n",
        "        cv2.fillPoly(mask, points, 1) \n",
        "        \n",
        "        if mode == 1:\n",
        "            for polygon in coordinates[1:]: \n",
        "                points = [np.int32([polygon])] \n",
        "                cv2.fillPoly(mask, points, 0) \n",
        "    return mask\n",
        "\n",
        "def parse_mask(shape: dict, image_size: tuple, mode) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Метод для парсинга фигур из geojson файла\n",
        "    \"\"\"\n",
        "    mask = np.zeros(image_size, dtype=np.float32)\n",
        "    coordinates = shape['coordinates']\n",
        "    if shape['type'] == 'MultiPolygon':\n",
        "        for polygon in coordinates:\n",
        "            mask += parse_polygon(polygon, image_size, mode)\n",
        "    else:\n",
        "        mask += parse_polygon(coordinates, image_size, mode)\n",
        "\n",
        "    return mask\n",
        "\n",
        "class_ids = {\"vessel\": 1}\n",
        "\n",
        "def read_layout(path: str, image_size: tuple, mode) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Метод для чтения geojson разметки и перевода в numpy маску\n",
        "    \"\"\"\n",
        "    with open(path, 'r', encoding='cp1251') as f:  # some files contain cyrillic letters, thus cp1251\n",
        "        json_contents = json.load(f)\n",
        "\n",
        "    num_channels = 1 + max(class_ids.values())\n",
        "    mask_channels = [np.zeros(image_size, dtype=np.float32) for _ in range(num_channels)]\n",
        "    mask = np.zeros(image_size, dtype=np.float32)\n",
        "\n",
        "    if type(json_contents) == dict and json_contents['type'] == 'FeatureCollection':\n",
        "        features = json_contents['features']\n",
        "    elif type(json_contents) == list:\n",
        "        features = json_contents\n",
        "    else:\n",
        "        features = [json_contents]\n",
        "\n",
        "    for shape in features:\n",
        "        channel_id = class_ids[\"vessel\"]\n",
        "        mask = parse_mask(shape['geometry'], image_size, mode)\n",
        "        mask_channels[channel_id] = np.maximum(mask_channels[channel_id], mask)\n",
        "\n",
        "    mask_channels[0] = 1 - np.max(mask_channels[1:], axis=0)\n",
        "\n",
        "    return np.stack(mask_channels, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv-kKR_2YBje"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ac20f20bb72b41158fa05627feca93de",
            "a0dfd15e127149ce88ced469a12dd5a6",
            "8f91d10935744169afb6dd9180080993",
            "e673a69af9e1493c9b6399afd28d59de",
            "4f2e485c392e403ebb286805386db90d",
            "513fd94f167140b7a2ee58be3d9485c4",
            "e5accd13803d4ecab670c8c541aceff9",
            "4fcd94b68aae400c8a9dad8cdbab1002",
            "75842ef1061445dd917f36646fb28f57",
            "7e2ff30d8ca04348af3f9ba0e0f92b3a",
            "791f9c4a5a9c4ba28212ccfc6dfefa2a"
          ]
        },
        "id": "AUulS2IVEIbF",
        "outputId": "d7e6fecd-8ad4-4609-e900-9052d504e1ab"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac20f20bb72b41158fa05627feca93de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1314 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "image_values = []\n",
        "mask_values_d = []\n",
        "mask_values_с = []\n",
        "paths_values = []\n",
        "\n",
        "for path in tqdm(sorted(os.listdir('train_folder'))):\n",
        "    if 'png' in path:\n",
        "        image = read_image('train_folder/' + path)\n",
        "        if  path.replace( \"png\", \"geojson\") not in os.listdir('train_folder'):\n",
        "            continue\n",
        "\n",
        "        mask_d = read_layout('train_folder/' + path.replace( \"png\", \"geojson\"), image.shape[:2], 0)\n",
        "        mask_c = read_layout('train_folder/' + path.replace( \"png\", \"geojson\"), image.shape[:2], 1)\n",
        "\n",
        "        image_values += [image]\n",
        "        mask_values_с += [mask_c]\n",
        "        mask_values_d += [mask_d]\n",
        "        paths_values += [path]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpw-tVNHDgxs"
      },
      "outputs": [],
      "source": [
        "skf = KFold(5, random_state=228, shuffle = True)\n",
        "split_list = []\n",
        "new_split_list = []\n",
        "num_i = 0\n",
        "for train_index, val_index in skf.split(np.arange(len(image_values)), np.arange(len(image_values)) ):\n",
        "    split_list += [(train_index, val_index)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwKVRcp8dhlj"
      },
      "outputs": [],
      "source": [
        "from coat import *\n",
        "from daformer import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8spS4mdpiZgp"
      },
      "outputs": [],
      "source": [
        "class RGB(nn.Module):\n",
        "    IMAGE_RGB_MEAN = [0.485, 0.456, 0.406]\n",
        "    # [0.485, 0.456, 0.406]  # [0.5, 0.5, 0.5]\n",
        "    IMAGE_RGB_STD = [0.229, 0.224, 0.225]\n",
        "    # [0.229, 0.224, 0.225]  # [0.5, 0.5, 0.5]\n",
        "\n",
        "    def __init__(self, ):\n",
        "        super(RGB, self).__init__()\n",
        "        self.register_buffer('mean', torch.zeros(1, 3, 1, 1))\n",
        "        self.register_buffer('std', torch.ones(1, 3, 1, 1))\n",
        "        self.mean.data = torch.FloatTensor(self.IMAGE_RGB_MEAN).view(self.mean.shape)\n",
        "        self.std.data = torch.FloatTensor(self.IMAGE_RGB_STD).view(self.std.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 encoder=coat_lite_medium,\n",
        "                 decoder=daformer_conv3x3,\n",
        "                 encoder_cfg={},\n",
        "                 decoder_cfg={},\n",
        "                 ):\n",
        "        super(Net, self).__init__()\n",
        "        decoder_dim = decoder_cfg.get('decoder_dim', 320)\n",
        "\n",
        "        # ----\n",
        "        self.rgb = RGB()\n",
        "\n",
        "        self.encoder = encoder\n",
        "\n",
        "        encoder_dim = self.encoder.embed_dims\n",
        "        # [64, 128, 320, 512]\n",
        "\n",
        "        self.decoder = decoder(\n",
        "            encoder_dim=encoder_dim,\n",
        "            decoder_dim=decoder_dim,\n",
        "        )\n",
        "        self.logit = nn.Sequential(\n",
        "            nn.Conv2d(decoder_dim, 1, kernel_size=1),\n",
        "        )\n",
        "        self.aux = nn.ModuleList([\n",
        "                    nn.Conv2d(decoder_dim, 1, kernel_size=1, padding=0) for i in range(4)\n",
        "                ])\n",
        "    def forward(self, batch):\n",
        "        x = batch\n",
        "        x = self.rgb(x)\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        encoder = self.encoder(x)\n",
        "        # print([f.shape for f in encoder])\n",
        "\n",
        "        last, decoder = self.decoder(encoder)\n",
        "        logit = self.logit(last)\n",
        "        # print(logit.shape)\n",
        "\n",
        "        output = {}\n",
        "        # probability_from_logit = torch.sigmoid(logit)\n",
        "        output['probability'] = logit\n",
        "        for i in range(4):\n",
        "            output[f'aux{i}'] = self.aux[i](decoder[i])\n",
        "\n",
        "        return output   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXEKxvL_rJS-"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riNI9o7MhS2A"
      },
      "outputs": [],
      "source": [
        "def transformer(p=1.0):\n",
        "    return A.Compose([\n",
        "        A.HorizontalFlip(p = 0.5),\n",
        "        # A.VerticalFlip(p = 0.5),\n",
        "        # A.RandomRotate90(p = 0.5),\n",
        "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.25, rotate_limit=15, p=0.25, \n",
        "                         border_mode=cv2.BORDER_CONSTANT),\n",
        "        A.ColorJitter(p = 0.5),\n",
        "        A.GaussNoise(15, p = 0.5),\n",
        "        # A.CLAHE(2, p =0.5)\n",
        "        A.OneOf([\n",
        "            A.OpticalDistortion(p=0.3),\n",
        "            A.GridDistortion(p=.1),\n",
        "            A.PiecewiseAffine(p=0.3),\n",
        "        ], p=0.3),\n",
        "        A.OneOf([\n",
        "            A.HueSaturationValue(10,15,10),\n",
        "            A.CLAHE(clip_limit=2),\n",
        "            A.RandomBrightnessContrast(),            \n",
        "        ], p=0.3),\n",
        "    ], p=p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QzU8xTlfOgh"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = 'train_images/'\n",
        "class HuBMAPDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, mask, imgs, imsize = (1440, 1088), tfms=None):\n",
        "        self.mask = mask\n",
        "        self.imgs = imgs\n",
        "        self.image_size = imsize\n",
        "        self.tfms = tfms\n",
        "        \n",
        "    def img2tensor(self, img,dtype:np.dtype=np.float32, mode =0 ):\n",
        "        if img.ndim==2 : img = np.expand_dims(img,2)\n",
        "        img = np.transpose(img,(2,0,1)) # C , H , W\n",
        "        return torch.from_numpy(img.astype(dtype, copy=False))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "    \n",
        "    def resize(self, img, interp):\n",
        "        return  cv2.resize(\n",
        "            img, self.image_size, interpolation=interp)\n",
        "    # (1632, 1248)\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.imgs[idx]\n",
        "        img = np.dstack([img, img, img])\n",
        "        mask = self.mask[idx]\n",
        "        # img = img.astype(np.float32)/255\n",
        "        if self.tfms is not None:\n",
        "            augmented = self.tfms(image=img,mask=mask)\n",
        "            img,mask = augmented['image'],augmented['mask']\n",
        "        img = img.astype(np.float32)/255\n",
        "        if len(mask.shape) == 3:\n",
        "            mask = mask[:, :, 1]\n",
        "        return self.img2tensor(self.resize(img , cv2.INTER_CUBIC)) , self.img2tensor(self.resize(mask , cv2.INTER_CUBIC))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqrwDDQfOPxO"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTNA5ntwjoDN"
      },
      "outputs": [],
      "source": [
        "DEVICE = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbzpHSR9j7OT"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmm43eMbzZEz"
      },
      "outputs": [],
      "source": [
        "def eval_nn(model, val_mask, val_dataloader, th, mode = 0):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        outputs_list = []\n",
        "        tk0 = tqdm(enumerate(val_dataloader), total = len(val_dataloader))\n",
        "        i = 0\n",
        "        loss_func = torch.nn.BCEWithLogitsLoss()\n",
        "        average_loss = 0\n",
        "        for batch_number,  (data)  in tk0:\n",
        "            # hui\n",
        "            img, mask = data\n",
        "            img = img.to(DEVICE)\n",
        "            mask = mask.to(DEVICE)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(img)['probability']  \n",
        "                loss = loss_func(ups(outputs), mask)\n",
        "                outputs =  torch.sigmoid(outputs)\n",
        "            \n",
        "            average_loss += loss.cpu().detach().numpy()\n",
        "            tk0.set_postfix(loss=average_loss / (batch_number + 1), stage=\"val\")\n",
        "            \n",
        "            for k in range(outputs.shape[0]):\n",
        "                sh = val_mask[i].shape\n",
        "                outputs_list += [torch.nn.functional.interpolate(outputs[k : k + 1], size=tuple(sh[:-1]),\n",
        "                                                    mode='bilinear',align_corners=False, antialias=True ).cpu().detach().numpy()]\n",
        "                i += 1\n",
        "        \n",
        "        # if mode == 1:\n",
        "        #     return outputs_list\n",
        "\n",
        "        list_score = []\n",
        "        i = 0\n",
        "        for true, pred in tqdm(zip(val_mask, outputs_list)):\n",
        "            list_score += [ score(true[:, :, 1], pred[0][0], th) ]\n",
        "        print(np.mean(list_score))\n",
        "        if mode == 1:\n",
        "            return outputs_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHEv82QiBdMo",
        "outputId": "948c28f1-79d4-457e-b7ba-7081bf531447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-09-29 20:18:51--  https://vcl.ucsd.edu/coat/pretrained/coat_lite_medium_384x384_f9129688.pth\n",
            "Resolving vcl.ucsd.edu (vcl.ucsd.edu)... 132.239.147.109\n",
            "Connecting to vcl.ucsd.edu (vcl.ucsd.edu)|132.239.147.109|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 535303289 (511M)\n",
            "Saving to: ‘coat_lite_medium_384x384_f9129688.pth’\n",
            "\n",
            "coat_lite_medium_38 100%[===================>] 510.50M   847KB/s    in 4m 31s  \n",
            "\n",
            "2022-09-29 20:23:23 (1.89 MB/s) - ‘coat_lite_medium_384x384_f9129688.pth’ saved [535303289/535303289]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://vcl.ucsd.edu/coat/pretrained/coat_lite_medium_384x384_f9129688.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xD9uA9Df1cQ"
      },
      "outputs": [],
      "source": [
        "def score(true, pred, th = 0.25):\n",
        "    pred = (pred > th).astype('int')\n",
        "    # print(pred_masks)\n",
        "    sm =( pred * true).sum()\n",
        "    if sm == 0:\n",
        "        return sm\n",
        "    rc = sm / (true.sum() + 1e-9)\n",
        "    ac = sm / (pred.sum() + 1e-9)\n",
        "    sc = 2 * (rc * ac) / ( rc + ac )\n",
        "    return sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVQQ-JdATFKM"
      },
      "outputs": [],
      "source": [
        "mask_values_c = [x.clip(0, 1) for x in mask_values_c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAeylppqpNAS"
      },
      "outputs": [],
      "source": [
        "drop_ids = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xNeIq_OzZ72I"
      },
      "outputs": [],
      "source": [
        "max_len = 1024\n",
        "batch_size = 1\n",
        "valid_batch_size = 1\n",
        "accumulation_steps = 1\n",
        "epochs = 24\n",
        "lr = 1.75e-5\n",
        "clip_grad_norm = 100\n",
        "params_train = {'batch_size': batch_size, 'shuffle': True, 'drop_last': False, 'num_workers': 4}\n",
        "params_valid = {'batch_size': valid_batch_size, 'shuffle': False, 'drop_last': False, 'num_workers': 4}\n",
        "ups = nn.Upsample(scale_factor = 4, mode='bilinear', align_corners=False)\n",
        "\n",
        "for fold in range(5):\n",
        "    train_mask = [mask_values_c[i] for i in split_list[fold][0] if i not in drop_ids] \n",
        "    train_img = [image_values[i] for i in split_list[fold][0] if i not in drop_ids] \n",
        "\n",
        "    train_dataset = HuBMAPDataset(train_mask, train_img, imsize = (1632, 1248), tfms = transformer())\n",
        "    train_dataloader = DataLoader(train_dataset, **params_train)\n",
        "\n",
        "    val_mask = [mask_values_c[i] for i in split_list[fold][1]]\n",
        "    val_img = [image_values[i] for i in split_list[fold][1]]\n",
        "\n",
        "    val_dataset = HuBMAPDataset(val_mask, val_img, imsize = (1632, 1248), tfms = None)\n",
        "    val_dataloader = DataLoader(val_dataset, **params_valid)\n",
        "\n",
        "    num_train_steps = int(len(train_dataset) / batch_size  * epochs)\n",
        "\n",
        "    encoder = coat_lite_medium()\n",
        "    checkpoint = 'drive/MyDrive/coat_lite_medium_a750cd63.pth'\n",
        "    # checkpoint = 'coat_lite_medium_384x384_f9129688.pth'\n",
        "    checkpoint = torch.load(checkpoint, map_location='cuda')\n",
        "    state_dict = checkpoint['model']\n",
        "    encoder.load_state_dict(state_dict,strict=False)\n",
        "\n",
        "    model = Net(encoder = encoder).cuda()\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), 5e-5)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, len(train_dataloader) * 8, 2, 1e-6)\n",
        "\n",
        "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
        "    # torch.nn.CrossEntropyLoss()\n",
        "    # loss_func = F.binary_cross_entropy_with_logits()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # if epoch != 0:\n",
        "        #     continue\n",
        "        model.train()\n",
        "        average_loss = 0\n",
        "        tk0 = tqdm(enumerate(train_dataloader), total = len(train_dataloader))\n",
        "        for batch_number,  (data)  in tk0:\n",
        "            # hui\n",
        "            optimizer.zero_grad()\n",
        "            img, mask = data\n",
        "            img = img.to(DEVICE)\n",
        "            mask = mask.to(DEVICE)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(img)\n",
        "                outputs_main = ups(outputs['probability'])\n",
        "                loss = loss_func(outputs_main, mask)\n",
        "                for i in range(2, 4):\n",
        "                    tmp_mask = F.interpolate(mask,size=outputs[f'aux{i}'].shape[-2:], mode='nearest')\n",
        "                    loss += 0.2 * loss_func(outputs[f'aux{i}'], tmp_mask)\n",
        "                    # loss += criterion_aux_loss(self.aux[i](decoder[i]),batch['mask'])   \n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            average_loss += loss.cpu().detach().numpy()\n",
        "            tk0.set_postfix(loss=average_loss / (batch_number + 1),lr = scheduler.get_last_lr()[0], stage=\"train\", epoch = epoch)\n",
        "\n",
        "        eval_nn(model, val_mask, val_dataloader, 0.425)\n",
        "\n",
        "        # torch.save({\n",
        "        #             'state_dict': model.state_dict()\n",
        "        #             # 'optimizer': optimizer.state_dict()\n",
        "        #                 }, f'drive/MyDrive/best_{fold}_{epoch}.pt')\n",
        "    \n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LthG6NZGdgzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6x_5nP1Lv_yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HuBMAPDatasetTEST(torch.utils.data.Dataset):\n",
        "    def __init__(self, imgs, imsize = 768, tfms=None):\n",
        "        # self.mask = mask\n",
        "        self.imgs = imgs\n",
        "        self.image_size = imsize\n",
        "        self.tfms = tfms\n",
        "        \n",
        "    def img2tensor(self, img,dtype:np.dtype=np.float32):\n",
        "        if img.ndim==2 : img = np.expand_dims(img,2)\n",
        "        img = np.transpose(img,(2,0,1)) # C , H , W\n",
        "        return torch.from_numpy(img.astype(dtype, copy=False))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "    \n",
        "    def resize(self, img, interp):\n",
        "        return  cv2.resize(\n",
        "            img,  (1632, 1248), interpolation=interp)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.imread(self.imgs[idx], cv2.IMREAD_COLOR)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        sh = img.shape\n",
        "        # img = img.astype(np.float32)/255\n",
        "        if self.tfms is not None:\n",
        "            augmented = self.tfms(image=img)\n",
        "            img = augmented['image']\n",
        "        img = img.astype(np.float32)/255\n",
        "        return self.img2tensor(self.resize(img , cv2.INTER_AREA)), sh"
      ],
      "metadata": {
        "id": "wspI2GpBv_0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_outputs_list = []\n",
        "for fold in range(5):\n",
        "    encoder = coat_lite_medium()\n",
        "    model = Net(encoder = encoder).cuda()\n",
        "    model_dict = torch.load(f'drive/MyDrive/best_{fold}_23.pt', map_location='cuda')\n",
        "    model.load_state_dict(model_dict['state_dict'])\n",
        "\n",
        "    batch_size = 1\n",
        "\n",
        "    params_valid = {'batch_size': batch_size, 'shuffle': False, 'drop_last': False, 'num_workers': 4}\n",
        "    ups = nn.Upsample(scale_factor = 4, mode='bilinear', align_corners=False)\n",
        "    list_test = ['test_folder/eye_test/' + x for x in os.listdir('test_folder/eye_test')]\n",
        "    test_dataset = HuBMAPDatasetTEST(list_test, tfms = None)\n",
        "    test_dataloader = DataLoader(test_dataset, **params_valid)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        outputs_list = []\n",
        "        tk0 = tqdm(enumerate(test_dataloader), total = len(test_dataloader))\n",
        "        loss_func = torch.nn.BCEWithLogitsLoss()\n",
        "        average_loss = 0\n",
        "        for batch_number,  (data)  in tk0:\n",
        "            img, shs = data\n",
        "            shs = np.array([shs[0].cpu().detach().numpy(), shs[1].cpu().detach().numpy()]).T\n",
        "            img = img.to(DEVICE)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(img)['probability']  \n",
        "                outputs =  torch.sigmoid(outputs)\n",
        "\n",
        "            i = 0\n",
        "            for k in range(outputs.shape[0]):\n",
        "                outputs_list += [torch.nn.functional.interpolate(outputs[k : k + 1], size=tuple(shs[i]),\n",
        "                                                    mode='bilinear',align_corners=False, antialias=True ).cpu().detach().numpy()]\n",
        "                i += 1\n",
        "\n",
        "    full_outputs_list += [outputs_list]"
      ],
      "metadata": {
        "id": "LCC6-CcX0dAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('ss')\n",
        "for i, f in tqdm(enumerate(list_test)):\n",
        "\n",
        "    pred = []\n",
        "    for k in range(5):\n",
        "        pred += [ full_outputs_list[k][i][0][0] ]\n",
        "    pred = np.mean(pred, 0)\n",
        "    ans = (pred > 0.425).astype(np.uint8) * 255\n",
        "    ans = np.array([ans, ans, ans])\n",
        "    if ans.sum() == 0:\n",
        "        hui\n",
        "    ans = np.transpose(ans, (1, 2, 0))\n",
        "    f = f.split('/')[-1]\n",
        "    cv2.imwrite(f'ss/{f}', ans)\n",
        "\n",
        "!zip -r -j drive/MyDrive/ss_final.zip ss/*"
      ],
      "metadata": {
        "id": "H4mHXFU80kF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f2e485c392e403ebb286805386db90d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fcd94b68aae400c8a9dad8cdbab1002": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "513fd94f167140b7a2ee58be3d9485c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75842ef1061445dd917f36646fb28f57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "791f9c4a5a9c4ba28212ccfc6dfefa2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e2ff30d8ca04348af3f9ba0e0f92b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f91d10935744169afb6dd9180080993": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fcd94b68aae400c8a9dad8cdbab1002",
            "max": 1314,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75842ef1061445dd917f36646fb28f57",
            "value": 1314
          }
        },
        "a0dfd15e127149ce88ced469a12dd5a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_513fd94f167140b7a2ee58be3d9485c4",
            "placeholder": "​",
            "style": "IPY_MODEL_e5accd13803d4ecab670c8c541aceff9",
            "value": "100%"
          }
        },
        "ac20f20bb72b41158fa05627feca93de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0dfd15e127149ce88ced469a12dd5a6",
              "IPY_MODEL_8f91d10935744169afb6dd9180080993",
              "IPY_MODEL_e673a69af9e1493c9b6399afd28d59de"
            ],
            "layout": "IPY_MODEL_4f2e485c392e403ebb286805386db90d"
          }
        },
        "e5accd13803d4ecab670c8c541aceff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e673a69af9e1493c9b6399afd28d59de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e2ff30d8ca04348af3f9ba0e0f92b3a",
            "placeholder": "​",
            "style": "IPY_MODEL_791f9c4a5a9c4ba28212ccfc6dfefa2a",
            "value": " 1314/1314 [05:43&lt;00:00,  2.68it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}